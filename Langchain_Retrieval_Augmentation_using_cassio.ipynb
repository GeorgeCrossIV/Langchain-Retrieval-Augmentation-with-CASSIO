{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GeorgeCrossIV/Langchain-Retrieval-Augmentation-with-CASSIO/blob/main/Langchain_Retrieval_Augmentation_using_cassio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLHkt-bMq4up"
      },
      "source": [
        "# Langchain Retrieval Augmentation (using Wikipedia data)\n",
        "Large Language Models (LLMs) have a data freshness problem. The most powerful LLMs in the world, like GPT-4, have no idea about recent world events.\n",
        "\n",
        "The world of LLMs is frozen in time. Their world exists as a static snapshot of the world as it was within their training data.\n",
        "\n",
        "A solution to this problem is retrieval augmentation. The idea behind this is that we retrieve relevant information from an external knowledge base and give that information to our LLM. In this notebook we will learn how to do that. In this demo, external or proprietary data will be stored in Astra DB and used to provide more current LLM responses."
      ],
      "id": "oLHkt-bMq4up"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQUeV_S5q4u0"
      },
      "source": [
        "## Colab-specific setup"
      ],
      "id": "WQUeV_S5q4u0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1p8iUgjq4u2"
      },
      "source": [
        "Make sure you have a Database and get ready to upload the Secure Connect Bundle and supply the Token string\n",
        "(see [Pre-requisites](https://cassio.org/start_here/#vector-database) on cassio.org for details).\n",
        "\n",
        "Likewise, ensure you have the necessary secret for the LLM provider of your choice: you'll be asked to input it shortly\n",
        "(see [Pre-requisites](https://cassio.org/start_here/#llm-access) on cassio.org for details).\n",
        "\n",
        "_Note: some portions of this notebook is part of the CassIO documentation. Visit [this page on cassIO.org](https://cassio.org/frameworks/langchain/qa-basic/)._\n"
      ],
      "id": "Z1p8iUgjq4u2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2953d95b",
      "metadata": {
        "id": "2953d95b"
      },
      "outputs": [],
      "source": [
        "# install required dependencies\n",
        "! pip install \\\n",
        "    \"git+https://github.com/hemidactylus/langchain@cassio#egg=langchain\" \\\n",
        "    \"cassandra-driver>=3.28.0\" \\\n",
        "    \"cassio>=0.0.4\" \\\n",
        "    \"google-cloud-aiplatform>=1.25.0\" \\\n",
        "    \"jupyter>=1.0.0\" \\\n",
        "    \"openai==0.27.7\" \\\n",
        "    \"python-dotenv==1.0.0\" \\\n",
        "    \"tensorflow-cpu==2.12.0\" \\\n",
        "    \"tiktoken==0.4.0\" \\\n",
        "    \"transformers>=4.29.2\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "222f44ff",
      "metadata": {
        "id": "222f44ff"
      },
      "source": [
        "You will likely be asked to \"Restart the Runtime\" at this time, as some dependencies\n",
        "have been upgraded. **Please do restart the runtime now** for a smoother execution from this point onward."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get the Wikipedia data from 20220301.simple"
      ],
      "metadata": {
        "id": "eZS2Xsy0WY-c"
      },
      "id": "eZS2Xsy0WY-c"
    },
    {
      "cell_type": "code",
      "source": [
        " !wget https://raw.githubusercontent.com/GeorgeCrossIV/Langchain-Retrieval-Augmentation-with-CASSIO/main/20220301.simple.csv"
      ],
      "metadata": {
        "id": "XrP5HE9RWhVg"
      },
      "id": "XrP5HE9RWhVg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the 20220301.simple wikipedia from the CSV file"
      ],
      "metadata": {
        "id": "Kte4Sybn2EOu"
      },
      "id": "Kte4Sybn2EOu"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('20220301.simple.csv')"
      ],
      "metadata": {
        "id": "SnIUf1Z3UD_5"
      },
      "id": "SnIUf1Z3UD_5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 10,000 entries in the Wikipedia data file. We'll reduce the dataset to 10 rows for this demo. It takes a while to process the data; however, feel free to increase the number of rows for future demo runs."
      ],
      "metadata": {
        "id": "YT7rxk7y1LK6"
      },
      "id": "YT7rxk7y1LK6"
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.head(10)\n",
        "data = data.rename(columns={'text ': 'text'})\n",
        "data"
      ],
      "metadata": {
        "id": "ffhIsRs1UQYz"
      },
      "id": "ffhIsRs1UQYz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will execute queries against the [Andouille](https://simple.wikipedia.org/wiki/Andouille) Wikipedia entries later in this demo. The Wikipedia data used in this demo is from a snapshot in time, stored in a CSV file. Below is the text of the Wikipedia record that will be processed."
      ],
      "metadata": {
        "id": "4xY-8xy61dME"
      },
      "id": "4xY-8xy61dME"
    },
    {
      "cell_type": "code",
      "source": [
        "data.iloc[9]['text']"
      ],
      "metadata": {
        "id": "5a9IQujzCkJ1"
      },
      "id": "5a9IQujzCkJ1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zh4P-XUDq4u9"
      },
      "outputs": [],
      "source": [
        "# Input your database keyspace name:\n",
        "ASTRA_DB_KEYSPACE = input('Your Astra DB Keyspace name: ')"
      ],
      "id": "zh4P-XUDq4u9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lThGqYchq4u-"
      },
      "outputs": [],
      "source": [
        "# Input your Astra DB token string, the one starting with \"AstraCS:...\"\n",
        "ASTRA_DB_TOKEN_BASED_PASSWORD = input('Your Astra DB Token: ')"
      ],
      "id": "lThGqYchq4u-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNQ6T_Gjk0Oz"
      },
      "source": [
        "### Astra DB Secure Connect Bundle\n",
        "\n",
        "Please upload the Secure Connect Bundle zipfile to connect to your Astra DB instance.\n",
        "\n",
        "The Secure Connect Bundle is needed to establish a secure connection to the database.\n",
        "Click [here](https://awesome-astra.github.io/docs/pages/astra/download-scb/#c-procedure) for instructions on how to download it from Astra DB."
      ],
      "id": "QNQ6T_Gjk0Oz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnNziXZ1q4vD"
      },
      "outputs": [],
      "source": [
        "# Upload your Secure Connect Bundle zipfile:\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "print('Please upload your Secure Connect Bundle')\n",
        "uploaded = files.upload()\n",
        "if uploaded:\n",
        "    astraBundleFileTitle = list(uploaded.keys())[0]\n",
        "    ASTRA_DB_SECURE_BUNDLE_PATH = os.path.join(os.getcwd(), astraBundleFileTitle)\n",
        "else:\n",
        "    raise ValueError(\n",
        "        'Cannot proceed without Secure Connect Bundle. Please re-run the cell.'\n",
        "    )"
      ],
      "id": "xnNziXZ1q4vD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUDw-07Iq4vE"
      },
      "outputs": [],
      "source": [
        "# colab-specific override of helper functions\n",
        "from cassandra.cluster import (\n",
        "    Cluster,\n",
        ")\n",
        "from cassandra.auth import PlainTextAuthProvider\n",
        "\n",
        "# The \"username\" is the literal string 'token' for this connection mode:\n",
        "ASTRA_DB_TOKEN_BASED_USERNAME = 'token'\n",
        "\n",
        "\n",
        "def getCQLSession(mode='astra_db'):\n",
        "    if mode == 'astra_db':\n",
        "        cluster = Cluster(\n",
        "            cloud={\n",
        "                \"secure_connect_bundle\": ASTRA_DB_SECURE_BUNDLE_PATH,\n",
        "            },\n",
        "            auth_provider=PlainTextAuthProvider(\n",
        "                ASTRA_DB_TOKEN_BASED_USERNAME,\n",
        "                ASTRA_DB_TOKEN_BASED_PASSWORD,\n",
        "            ),\n",
        "        )\n",
        "        astraSession = cluster.connect()\n",
        "        return astraSession\n",
        "    else:\n",
        "        raise ValueError('Unsupported CQL Session mode')\n",
        "\n",
        "def getCQLKeyspace(mode='astra_db'):\n",
        "    if mode == 'astra_db':\n",
        "        return ASTRA_DB_KEYSPACE\n",
        "    else:\n",
        "        raise ValueError('Unsupported CQL Session mode')\n",
        "\n",
        "def getTableCount():\n",
        "  # create a query that counts the number of records of the Astra DB table\n",
        "  query = SimpleStatement(f\"\"\"SELECT COUNT(*) FROM {keyspace}.{table_name};\"\"\")\n",
        "\n",
        "  # execute the query\n",
        "  results = session.execute(query)\n",
        "  return results.one().count"
      ],
      "id": "TUDw-07Iq4vE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXCQ6T_Gjk0Oz"
      },
      "source": [
        "### LLM Provider\n",
        "\n",
        "In the cell below you can choose between **GCP VertexAI** or **OpenAI** for your LLM services.\n",
        "(See [Pre-requisites](https://cassio.org/start_here/#llm-access) on cassio.org for more details).\n",
        "\n",
        "Make sure you set the `llmProvider` variable and supply the corresponding access secrets in the following cell."
      ],
      "id": "QXCQ6T_Gjk0Oz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGpzZc5zq4vG"
      },
      "outputs": [],
      "source": [
        "# Set your secret(s) for LLM access:\n",
        "llmProvider = 'OpenAI'  # 'GCP_VertexAI'\n"
      ],
      "id": "pGpzZc5zq4vG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOFStlEAq4vH"
      },
      "outputs": [],
      "source": [
        "if llmProvider == 'OpenAI':\n",
        "    apiSecret = input(f'Your secret for LLM provider \"{llmProvider}\": ')\n",
        "    os.environ['OPENAI_API_KEY'] = apiSecret\n",
        "elif llmProvider == 'GCP_VertexAI':\n",
        "    # we need a json file\n",
        "    print(f'Please upload your Service Account JSON for the LLM provider \"{llmProvider}\":')\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    if uploaded:\n",
        "        vertexAIJsonFileTitle = list(uploaded.keys())[0]\n",
        "        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = os.path.join(os.getcwd(), vertexAIJsonFileTitle)\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            'No file uploaded. Please re-run the cell.'\n",
        "        )\n",
        "else:\n",
        "    raise ValueError('Unknown/unsupported LLM Provider')"
      ],
      "id": "zOFStlEAq4vH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0rYQTmwq4vI"
      },
      "source": [
        "### Colab preamble completed\n",
        "\n",
        "The following cells constitute the demo notebook proper."
      ],
      "id": "W0rYQTmwq4vI"
    },
    {
      "cell_type": "markdown",
      "id": "6715bc2b",
      "metadata": {
        "id": "6715bc2b"
      },
      "source": [
        "# Vector Similarity Search QA Quickstart"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "761d9b70",
      "metadata": {
        "id": "761d9b70"
      },
      "source": [
        "_**NOTE:** this uses Cassandra's \"Vector Similarity Search\" capability.\n",
        "Make sure you are connecting to a vector-enabled database for this demo._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "042f832e",
      "metadata": {
        "id": "042f832e"
      },
      "outputs": [],
      "source": [
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain.text_splitter import (\n",
        "    CharacterTextSplitter,\n",
        "    RecursiveCharacterTextSplitter,\n",
        ")\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
        "from cassandra.query import SimpleStatement"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4388ac1d",
      "metadata": {
        "id": "4388ac1d"
      },
      "source": [
        "The following line imports the Cassandra flavor of a LangChain vector store:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d65c46f0",
      "metadata": {
        "id": "d65c46f0"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores.cassandra import Cassandra"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4578a87b",
      "metadata": {
        "id": "4578a87b"
      },
      "source": [
        "A database connection is needed to access Cassandra. The following assumes\n",
        "that a _vector-search-capable Astra DB instance_ is available. Adjust as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11013224",
      "metadata": {
        "id": "11013224"
      },
      "outputs": [],
      "source": [
        "# creation of the DB connection\n",
        "cqlMode = 'astra_db'\n",
        "session = getCQLSession(mode=cqlMode)\n",
        "keyspace = getCQLKeyspace(mode=cqlMode)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32e2a156",
      "metadata": {
        "id": "32e2a156"
      },
      "source": [
        "Both an LLM and an embedding function are required.\n",
        "\n",
        "Below is the logic to instantiate the LLM and embeddings of choice. We choose to leave it in the notebooks for clarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "124e3de4",
      "metadata": {
        "id": "124e3de4"
      },
      "outputs": [],
      "source": [
        "# creation of the LLM resources\n",
        "\n",
        "if llmProvider == 'GCP_VertexAI':\n",
        "    from langchain.llms import VertexAI\n",
        "    from langchain.embeddings import VertexAIEmbeddings\n",
        "    llm = VertexAI()\n",
        "    myEmbedding = VertexAIEmbeddings()\n",
        "    print('LLM+embeddings from VertexAI')\n",
        "elif llmProvider == 'OpenAI':\n",
        "    from langchain.llms import OpenAI\n",
        "    from langchain.embeddings import OpenAIEmbeddings\n",
        "    llm = OpenAI(temperature=0)\n",
        "    myEmbedding = OpenAIEmbeddings()\n",
        "    print('LLM+embeddings from OpenAI')\n",
        "else:\n",
        "    raise ValueError('Unknown LLM provider.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "285f29cf",
      "metadata": {
        "id": "285f29cf"
      },
      "source": [
        "## Langchain Retrieval Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cf74a31",
      "metadata": {
        "id": "5cf74a31"
      },
      "source": [
        "The following is a minimal usage of the Cassandra vector store. The store is created and filled at once, and is then queried to retrieve relevant parts of the indexed text, which are then stuffed into a prompt finally used to answer a question."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f29fc57",
      "metadata": {
        "id": "6f29fc57"
      },
      "source": [
        "The following creates an \"index creator\", which knows about the type of vector store, the embedding to use and how to preprocess the input text:\n",
        "\n",
        "_(Note: stores built with different embedding functions will need different tables. This is why we append the `llmProvider` name to the table name in the next cell.)_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2cfe71b",
      "metadata": {
        "id": "d2cfe71b"
      },
      "outputs": [],
      "source": [
        "table_name = 'vs_test1_' + llmProvider\n",
        "\n",
        "index_creator = VectorstoreIndexCreator(\n",
        "    vectorstore_cls=Cassandra,\n",
        "    embedding=myEmbedding,\n",
        "    text_splitter=CharacterTextSplitter(\n",
        "        chunk_size=400,\n",
        "        chunk_overlap=0,\n",
        "    ),\n",
        "    vectorstore_kwargs={\n",
        "        'session': session,\n",
        "        'keyspace': keyspace,\n",
        "        'table_name': table_name,\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the Cassandra Vector Store and clear entries if the table already exists"
      ],
      "metadata": {
        "id": "C7V7uPgvE3yY"
      },
      "id": "C7V7uPgvE3yY"
    },
    {
      "cell_type": "code",
      "source": [
        "myCassandraVStore = Cassandra(\n",
        "    embedding=myEmbedding,\n",
        "    session=session,\n",
        "    keyspace=keyspace,\n",
        "    table_name='vs_test1_' + llmProvider,\n",
        ")\n",
        "\n",
        "myCassandraVStore.clear()"
      ],
      "metadata": {
        "id": "SCxWxjRWl8Dg"
      },
      "id": "SCxWxjRWl8Dg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mySplitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=120)"
      ],
      "metadata": {
        "id": "6ITWD1pqmgeu"
      },
      "id": "6ITWD1pqmgeu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Astra DB table should be cleared from the last statement. Let's do a quick table count to make sure."
      ],
      "metadata": {
        "id": "AFQxGLE_D4Ti"
      },
      "id": "AFQxGLE_D4Ti"
    },
    {
      "cell_type": "code",
      "source": [
        "# create a query that counts the number of records of the Astra DB table\n",
        "print('Total records: ' + str(getTableCount()))"
      ],
      "metadata": {
        "id": "_v6hqPI_EDp4"
      },
      "id": "_v6hqPI_EDp4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the function for creating a vector index for a Wikipedia entry"
      ],
      "metadata": {
        "id": "oogCCCXcv3yG"
      },
      "id": "oogCCCXcv3yG"
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vector_index(row, myCassandraVStore):\n",
        "  metadata = {\n",
        "    'url': row['url'],\n",
        "    'title': row['title']\n",
        "  }\n",
        "  page_content = row['text']\n",
        "\n",
        "  wikiDocument = Document(\n",
        "      page_content=page_content,\n",
        "      metadata=metadata\n",
        "  )\n",
        "  wikiDocs = mySplitter.transform_documents([wikiDocument])\n",
        "  myCassandraVStore.add_documents(wikiDocs)"
      ],
      "metadata": {
        "id": "mbsblXxQwAT9"
      },
      "id": "mbsblXxQwAT9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute the create_vector_index function for each row in the Wikipedia dataframe. It's good time to grab a drink as the next step will take about 90 seconds to complete."
      ],
      "metadata": {
        "id": "ZCXx5rKUxj_z"
      },
      "id": "ZCXx5rKUxj_z"
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in data.iterrows():\n",
        "  create_vector_index(row, myCassandraVStore)"
      ],
      "metadata": {
        "id": "M3K5EOR1xrdO"
      },
      "id": "M3K5EOR1xrdO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've processed records by embedding them with vector search values, let's see how many records are in the Astra DB table. We'll also look at one of the rows and a snippet of it's new vector values"
      ],
      "metadata": {
        "id": "f4QNwcMuJAXh"
      },
      "id": "f4QNwcMuJAXh"
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number records of the Astra DB table after the embedding\n",
        "print('Total records: ' + str(getTableCount()))"
      ],
      "metadata": {
        "id": "luCa9nvqNdXm"
      },
      "id": "luCa9nvqNdXm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a query that returns the first row of the Astra DB table\n",
        "query = SimpleStatement(f\"\"\"SELECT * from {keyspace}.{table_name} limit 1;\"\"\")\n",
        "\n",
        "# execute the query and display the first row of the table\n",
        "results = session.execute(query)\n",
        "print('document_id: ' + results.one().document_id)\n",
        "print('document: ' + results.one().document)\n",
        "print('metadata_blob: ' + results.one().metadata_blob)\n",
        "print('first 20 bytes of embedding_vector: ' + str(results.one().embedding_vector[:20]))"
      ],
      "metadata": {
        "id": "pQUQd1uSNe2G"
      },
      "id": "pQUQd1uSNe2G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndexWrapper(vectorstore=myCassandraVStore)"
      ],
      "metadata": {
        "id": "fySPtKkYmAgh"
      },
      "id": "fySPtKkYmAgh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's query our proprietory store. We'll ask \"What is Andouille?\""
      ],
      "metadata": {
        "id": "geA-eXncFwvi"
      },
      "id": "geA-eXncFwvi"
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is Andouille?\"\n",
        "index.query(query,llm=llm)"
      ],
      "metadata": {
        "id": "cEp-I8wd5Abv"
      },
      "id": "cEp-I8wd5Abv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm really interested in what temperature to cook my andouille."
      ],
      "metadata": {
        "id": "tRiU4IMkF6di"
      },
      "id": "tRiU4IMkF6di"
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What temperature should Andouile be cooked?\"\n",
        "index.query(query,llm=llm)"
      ],
      "metadata": {
        "id": "S_96yaY45OFw"
      },
      "id": "S_96yaY45OFw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare this answer to what OpenAi GPT-3 will return"
      ],
      "metadata": {
        "id": "4uJebWz9JfAu"
      },
      "id": "4uJebWz9JfAu"
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key = apiSecret\n",
        "response = openai.Completion.create(\n",
        "  engine=\"text-davinci-003\",\n",
        "  prompt=\"What temperature should Andouille be cooked?\",\n",
        "  max_tokens=100\n",
        ")\n",
        "\n",
        "print(response.choices[0].text.strip())"
      ],
      "metadata": {
        "id": "zX48Pu3-Jl9B"
      },
      "id": "zX48Pu3-Jl9B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You've now seen how we can use a LLM to answer the prompt from our Astra Vector Store, but notice that the answer is different from using the LLM directly."
      ],
      "metadata": {
        "id": "GmIdlZngKvtU"
      },
      "id": "GmIdlZngKvtU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's get some information about the source for the response to the question \"What temperature should Andouille be cooked?\""
      ],
      "metadata": {
        "id": "DikzwEj0Gawb"
      },
      "id": "DikzwEj0Gawb"
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = index.vectorstore.as_retriever(search_kwargs={\n",
        "    'k': 2,\n",
        "})"
      ],
      "metadata": {
        "id": "a5MrZWD7uaBa"
      },
      "id": "a5MrZWD7uaBa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.get_relevant_documents(\n",
        "    \"What temperature should Andouille be cooked?\"\n",
        ")"
      ],
      "metadata": {
        "id": "dg1FUbYoudSr"
      },
      "id": "dg1FUbYoudSr",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}